{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP+Sl3SAts+55E9WeFzB/ha"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0fafbd73c73e4af8b6c047d3a7e82238":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cb014f50e0ad40c08aac2aa5b2089baf","IPY_MODEL_2af697f326f441e794e2d655958d35fd","IPY_MODEL_39dc1d6563054db3a95fdea57967827b"],"layout":"IPY_MODEL_908191413e614728acdf8ba0fc67248c"}},"cb014f50e0ad40c08aac2aa5b2089baf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e72965d207940e3ba0557bc1e21585a","placeholder":"‚Äã","style":"IPY_MODEL_ea7159d4e0634aa2912221d5f26ef07f","value":"100%"}},"2af697f326f441e794e2d655958d35fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98fad9ba2d69415ab725286b23590c8d","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bdb39226a51a4e92a9b2357652227ff2","value":20}},"39dc1d6563054db3a95fdea57967827b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c105eb1c89ad478c90156059593f80c0","placeholder":"‚Äã","style":"IPY_MODEL_2ff14b9099104e3fa391385ab9d5cd5b","value":"‚Äá20/20‚Äá[00:05&lt;00:00,‚Äá‚Äá4.77it/s]"}},"908191413e614728acdf8ba0fc67248c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e72965d207940e3ba0557bc1e21585a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea7159d4e0634aa2912221d5f26ef07f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98fad9ba2d69415ab725286b23590c8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdb39226a51a4e92a9b2357652227ff2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c105eb1c89ad478c90156059593f80c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ff14b9099104e3fa391385ab9d5cd5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3sHH77qmCt4","executionInfo":{"status":"ok","timestamp":1770115037872,"user_tz":-240,"elapsed":22944,"user":{"displayName":"Tamar Vanishvili","userId":"05982125587332184201"}},"outputId":"c3ac335f-0382-4663-bb29-094a6049cc69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import sys\n","sys.path.append('.')\n","from torch import nn\n","import pickle\n","import torch.nn.functional as F\n","import torch\n","from torch.utils.data import DataLoader, random_split\n","import time\n","import os\n","from sklearn import metrics\n","import numpy as np\n","from google.colab import drive\n","import os\n","\n","# 1. Mount Drive\n","drive.mount('/content/drive')\n","root_path = '/content/drive/MyDrive/AMP-Generation/filtering'\n","os.chdir(root_path)\n","sys.path.append(root_path)\n","\n","from src.RCNN import RCNNModel\n","\n","with open('params/peptide_vocab.pkl', 'rb') as f:\n","    w2i = pickle.load(f)\n","import random\n","\n","def split_file(input_file, train_file, test_file, split_ratio=0.9):\n","    with open(input_file, 'r', encoding='utf-8') as f:\n","        lines = f.readlines()\n","    random.shuffle(lines)\n","    split_point = int(len(lines) * split_ratio)\n","    train_lines = lines[:split_point]\n","    test_lines = lines[split_point:]\n","    with open(train_file, 'w', encoding='utf-8') as f:\n","        f.writelines(train_lines)\n","    with open(test_file, 'w', encoding='utf-8') as f:\n","        f.writelines(test_lines)\n","os.makedirs(f\"params/train\",exist_ok=True)\n","os.makedirs(f\"params/test\",exist_ok=True)\n","split_file(\"params/pos_data\", f\"params/train/pos\", f\"params/test/pos\")\n","split_file(\"params/neg_data\", f\"params/train/neg\", f\"params/test/neg\")\n","split_file(\"params/neg_data_10\", f\"params/train/neg_10\", f\"params/test/neg_10\")"]},{"cell_type":"code","source":["def train_model(model, save_path, negname, lr):\n","    num_epochs = 100\n","    patience = 15\n","    counter = 0\n","\n","    # Create unique directories for this specific run\n","    os.makedirs(f\"./{save_path}\", exist_ok=True)\n","\n","    # Data Loading\n","    all_data = MyDataset_class(f\"params/train/pos\", f\"params/train/{negname}\")\n","    train_num = int(len(all_data) * 0.85)\n","    val_num = len(all_data) - train_num\n","    train_set, val_set = random_split(all_data, [train_num, val_num])\n","\n","    train_loader = DataLoader(dataset=train_set, batch_size=512, shuffle=True, drop_last=True)\n","    val_loader = DataLoader(dataset=val_set, batch_size=256, shuffle=False)\n","\n","    print(f\"\\n>>>> INITIALIZING: {save_path}\")\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n","    best_acc_val = 0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_losses = []\n","        for data, labels in train_loader:\n","            optimizer.zero_grad()\n","            output = model(data.cuda())\n","            loss = F.cross_entropy(output, labels.cuda())\n","            loss.backward()\n","            optimizer.step()\n","            train_losses.append(loss.item())\n","\n","        scheduler.step()\n","\n","        # Validation\n","        model.eval()\n","        predict_all, labels_all = [], []\n","        with torch.no_grad():\n","            for data, labels in val_loader:\n","                output = model(data.cuda())\n","                predic = torch.max(output.data, 1)[1].cpu().numpy()\n","                predict_all.extend(predic)\n","                labels_all.extend(labels.numpy())\n","\n","        val_acc = metrics.accuracy_score(labels_all, predict_all)\n","\n","        # CHECKPOINT 1: Save the \"Best\" weights\n","        if val_acc > best_acc_val:\n","            best_acc_val = val_acc\n","            counter = 0\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'acc': val_acc,\n","            }, f\"./{save_path}/best_model.pth\")\n","\n","            # Save logs\n","            val_report = metrics.classification_report(labels_all, predict_all, target_names=[\"AMP\", \"NAMP\"], digits=4)\n","            with open(f\"./{save_path}/model_data\", \"w\") as wf:\n","                wf.write(f\"Best Epoch: {epoch}\\nVal Acc: {val_acc}\\n\\n{val_report}\")\n","        else:\n","            counter += 1\n","\n","        # CHECKPOINT 2: Save \"Latest\" weights every epoch (Safety Net)\n","        # This overwrites itself so you don't fill up your Google Drive\n","        torch.save(model.state_dict(), f\"./{save_path}/latest_checkpoint.pth\")\n","\n","        if epoch % 5 == 0:\n","            print(f\"Epoch {epoch:03d} | Val Acc: {val_acc:.4f} | Best: {best_acc_val:.4f} | Patience: {counter}/{patience}\")\n","\n","        if counter >= patience:\n","            print(f\"!! Early Stopping at epoch {epoch}. Model saved to {save_path}\")\n","            break"],"metadata":{"id":"LbFpTfiznBIa","executionInfo":{"status":"ok","timestamp":1770115074498,"user_tz":-240,"elapsed":4,"user":{"displayName":"Tamar Vanishvili","userId":"05982125587332184201"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# List of models to train\n","model_factories = [\n","    (\"CNN\", CNNModel),\n","    (\"RNN\", RNNModel),\n","    (\"RNN_atten\", RNN_attenModel),\n","    (\"RCNN\", RCNNModel),\n","    (\"Transformer\", TransformerModel)\n","]\n","\n","run_lr = [0.01, 0.001, 0.0001, 0.00001, 0.000001]\n","#run_lr = [0.0001, 0.00001, 0.000001]\n","\n","neg_types = [\"neg\", \"neg_10\"]\n","\n","for model_name, model_class in model_factories:\n","    for lr in run_lr:\n","        for neg in neg_types:\n","            # Create a unique path: e.g., \"Checkpoints/CNN/neg_10/LR_0.001\"\n","            save_dir = f\"Ensemble_Storage/{model_name}/{neg}/LR_{lr}\"\n","\n","            # Initialize a fresh model\n","            m = model_class().cuda()\n","\n","            # Train and save\n","            train_model(m, save_dir, neg, lr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1e34Y7EenXrv","outputId":"c8c292cb-6aa3-4b35-d877-cb065fb02038"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n",">>>> INITIALIZING: Ensemble_Storage/RCNN/neg/LR_0.01\n","Epoch 000 | Val Acc: 0.8684 | Best: 0.8684 | Patience: 0/15\n","Epoch 005 | Val Acc: 0.9216 | Best: 0.9216 | Patience: 0/15\n","Epoch 010 | Val Acc: 0.9230 | Best: 0.9271 | Patience: 1/15\n","Epoch 015 | Val Acc: 0.9216 | Best: 0.9271 | Patience: 6/15\n","Epoch 020 | Val Acc: 0.9270 | Best: 0.9271 | Patience: 11/15\n","!! Early Stopping at epoch 24. Model saved to Ensemble_Storage/RCNN/neg/LR_0.01\n","\n",">>>> INITIALIZING: Ensemble_Storage/RCNN/neg_10/LR_0.01\n","Epoch 000 | Val Acc: 0.9265 | Best: 0.9265 | Patience: 0/15\n","Epoch 005 | Val Acc: 0.9612 | Best: 0.9612 | Patience: 0/15\n","Epoch 010 | Val Acc: 0.9572 | Best: 0.9650 | Patience: 1/15\n","Epoch 015 | Val Acc: 0.9662 | Best: 0.9662 | Patience: 0/15\n","Epoch 020 | Val Acc: 0.9670 | Best: 0.9677 | Patience: 4/15\n","Epoch 025 | Val Acc: 0.9679 | Best: 0.9679 | Patience: 0/15\n","Epoch 030 | Val Acc: 0.9665 | Best: 0.9685 | Patience: 3/15\n","Epoch 035 | Val Acc: 0.9688 | Best: 0.9688 | Patience: 0/15\n","Epoch 040 | Val Acc: 0.9703 | Best: 0.9703 | Patience: 0/15\n","Epoch 045 | Val Acc: 0.9674 | Best: 0.9703 | Patience: 5/15\n","Epoch 050 | Val Acc: 0.9693 | Best: 0.9703 | Patience: 10/15\n","Epoch 055 | Val Acc: 0.9658 | Best: 0.9703 | Patience: 15/15\n","!! Early Stopping at epoch 55. Model saved to Ensemble_Storage/RCNN/neg_10/LR_0.01\n","\n",">>>> INITIALIZING: Ensemble_Storage/RCNN/neg/LR_0.001\n","Epoch 000 | Val Acc: 0.8810 | Best: 0.8810 | Patience: 0/15\n","Epoch 005 | Val Acc: 0.9253 | Best: 0.9253 | Patience: 0/15\n","Epoch 010 | Val Acc: 0.9314 | Best: 0.9316 | Patience: 2/15\n","Epoch 015 | Val Acc: 0.9265 | Best: 0.9323 | Patience: 3/15\n","Epoch 020 | Val Acc: 0.9285 | Best: 0.9333 | Patience: 3/15\n"]}]},{"cell_type":"code","source":["import os\n","import re\n","\n","def find_best_lrs(root_dir=\"Ensemble_Storage\"):\n","    # This will store architecture -> (best_accuracy, lr_string)\n","    best_tracker = {}\n","\n","    # Architecture names mapped to their folder names and negative types\n","    # Mapping \"CNN_10-fold\" to look in the \"neg_10\" folder\n","    targets = {\n","        \"CNN\": (\"CNN\", \"neg\"),\n","        \"RCNN\": (\"RCNN\", \"neg\"),\n","        \"RNN\": (\"RNN\", \"neg\"),\n","        \"RNN_atten\": (\"RNN_atten\", \"neg\"),\n","        \"Transformer\": (\"Transformer\", \"neg\"),\n","        \"CNN_10-fold\": (\"CNN\", \"neg_10\"),\n","        \"RCNN_10-fold\": (\"RCNN\", \"neg_10\"),\n","        \"RNN_10-fold\": (\"RNN\", \"neg_10\"),\n","        \"RNN_atten_10-fold\": (\"RNN_atten\", \"neg_10\"),\n","        \"Transformer_10-fold\": (\"Transformer\", \"neg_10\"),\n","    }\n","\n","    for display_name, (folder_name, neg_type) in targets.items():\n","        search_path = os.path.join(root_dir, folder_name, neg_type)\n","        best_acc = -1.0\n","        best_lr = None\n","\n","        if not os.path.exists(search_path):\n","            print(f\"‚ö†Ô∏è Directory missing: {search_path}\")\n","            continue\n","\n","        # Look into each LR folder (e.g., LR_0.001)\n","        for lr_folder in os.listdir(search_path):\n","            if not lr_folder.startswith(\"LR_\"): continue\n","\n","            data_file = os.path.join(search_path, lr_folder, \"model_data\")\n","            if os.path.exists(data_file):\n","                with open(data_file, \"r\") as f:\n","                    content = f.read()\n","                    # Use regex to find \"Val Acc: 0.95\"\n","                    match = re.search(r\"Val Acc:\\s+([0-9.]+)\", content)\n","                    if match:\n","                        acc = float(match.group(1))\n","                        if acc > best_acc:\n","                            best_acc = acc\n","                            # Extract just the number from \"LR_0.001\"\n","                            best_lr = lr_folder.replace(\"LR_\", \"\")\n","\n","        best_tracker[display_name] = best_lr\n","\n","    # Print in the exact format you requested\n","    print(\"\\nbest_run_lr = {\")\n","    for key, value in best_tracker.items():\n","        # Handle the case where a number might be in scientific notation string\n","        formatted_val = f\"'{value}'\" if value is not None else \"None\"\n","        print(f\"    \\\"{key}\\\": {formatted_val},\")\n","    print(\"}\")\n","\n","    return best_tracker\n","\n","# Execute\n","best_run_lr_dict = find_best_lrs()"],"metadata":{"id":"L6sQC6zAs1Tu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from sklearn import metrics\n","\n","def predict(data,model,predictor,vote=True):\n","    out = []\n","    if \"CNN\" in model:\n","        if vote:\n","            output = predictor[\"CNN\"](data)\n","            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n","            out.append(predic)\n","        else:\n","            out.append(predictor[\"CNN\"](data).unsqueeze(0))\n","    if \"RCNN\" in model:\n","        if vote:\n","            output = predictor[\"RCNN\"](data)\n","            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n","            out.append(predic)\n","        else:\n","            out.append(predictor[\"RCNN\"](data).unsqueeze(0))\n","    if \"RNN\" in model:\n","        if vote:\n","            output = predictor[\"RNN\"](data)\n","            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n","            out.append(predic)\n","        else:\n","            out.append(predictor[\"RNN\"](data).unsqueeze(0))\n","    if \"RNN_atten\" in model:\n","        if vote:\n","            output = predictor[\"RNN_atten\"](data)\n","            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n","            out.append(predic)\n","        else:\n","            out.append(predictor[\"RNN_atten\"](data).unsqueeze(0))\n","    if \"Transformer\" in model:\n","        if vote:\n","            output = predictor[\"Transformer\"](data)\n","            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n","            out.append(predic)\n","        else:\n","            out.append(predictor[\"Transformer\"](data).unsqueeze(0))\n","    if \"CNN_10-fold\" in model:\n","        if vote:\n","            output = predictor[\"CNN_10-fold\"](data)\n","            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n","            out.append(predic)\n","        else:\n","            out.append(predictor[\"CNN_10-fold\"](data).unsqueeze(0))\n","    if \"RCNN_10-fold\" in model:\n","        if vote:\n","            output = predictor[\"RCNN_10-fold\"](data)\n","            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n","            out.append(predic)\n","        else:\n","            out.append(predictor[\"RCNN_10-fold\"](data).unsqueeze(0))\n","    if \"RNN_10-fold\" in model:\n","        if vote:\n","            output = predictor[\"RNN_10-fold\"](data)\n","            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n","            out.append(predic)\n","        else:\n","            out.append(predictor[\"RNN_10-fold\"](data).unsqueeze(0))\n","    if \"RNN_atten_10-fold\" in model:\n","        if vote:\n","            output = predictor[\"RNN_atten_10-fold\"](data)\n","            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n","            out.append(predic)\n","        else:\n","            out.append(predictor[\"RNN_atten_10-fold\"](data).unsqueeze(0))\n","    if \"Transformer_10-fold\" in model:\n","        if vote:\n","            output = predictor[\"Transformer_10-fold\"](data)\n","            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n","            out.append(predic)\n","        else:\n","            out.append(predictor[\"Transformer_10-fold\"](data).unsqueeze(0))\n","    if vote:\n","        out = (torch.concat(out).sum(0) > int(len(model)/2 -1)).int()\n","    else:\n","        out = torch.concat(out).mean(0)\n","        out = torch.max(out.data, 1)[1].cpu().unsqueeze(0).numpy()\n","    return out"],"metadata":{"id":"5Ir_iICWpLnc","executionInfo":{"status":"ok","timestamp":1770115086542,"user_tz":-240,"elapsed":79,"user":{"displayName":"Tamar Vanishvili","userId":"05982125587332184201"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["best_run_lr = {\n","    \"CNN\": '0.001',\n","    \"RCNN\": '0.0001',\n","    \"RNN\": '0.0001',\n","    \"RNN_atten\": '0.001',\n","    \"Transformer\": '0.001',\n","    \"CNN_10-fold\": '0.001',\n","    \"RCNN_10-fold\": '0.001',\n","    \"RNN_10-fold\": '0.001',\n","    \"RNN_atten_10-fold\": '0.001',\n","    \"Transformer_10-fold\": '0.0001',\n","}"],"metadata":{"id":"M42oIZNtvls1","executionInfo":{"status":"ok","timestamp":1770115089082,"user_tz":-240,"elapsed":21,"user":{"displayName":"Tamar Vanishvili","userId":"05982125587332184201"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from src.Transformer import MyDataset_class\n","# 1. Re-initialize the Dataset and Loader\n","test_set = MyDataset_class(\"params/test/pos\", \"params/test/neg_10\")\n","test_loader = DataLoader(\n","    dataset=test_set,\n","    batch_size=256,\n","    shuffle=False,  # Set to False for consistent indexing during pre-calculation\n","    drop_last=False,\n","    num_workers=0,\n","    pin_memory=True\n",")\n","\n","# 2. Make sure tqdm is imported\n","from tqdm.auto import tqdm"],"metadata":{"id":"NnWYWd8Nctn1","executionInfo":{"status":"ok","timestamp":1770115090850,"user_tz":-240,"elapsed":288,"user":{"displayName":"Tamar Vanishvili","userId":"05982125587332184201"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import pandas as pd\n","import os\n","from itertools import combinations\n","from sklearn import metrics\n","from tqdm.auto import tqdm\n","from torch.utils.data import DataLoader\n","\n","from src.RCNN import RCNNModel\n","from src.CNN import CNNModel\n","from src.RNN import RNNModel\n","from src.RNN_atten import RNN_attenModel\n","from src.Transformer import TransformerModel\n","\n","# --- 0. CONFIGURATION & DEVICE ---\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using Device: {device}\")\n","\n","# --- 1. INITIALIZE & LOAD MODELS ---\n","print(\"üì¶ Loading models and moving to GPU...\")\n","predictor = {\n","    \"CNN\": CNNModel(), \"RCNN\": RCNNModel(), \"RNN\": RNNModel(),\n","    \"RNN_atten\": RNN_attenModel(), \"Transformer\": TransformerModel(),\n","    \"CNN_10-fold\": CNNModel(), \"RCNN_10-fold\": RCNNModel(), \"RNN_10-fold\": RNNModel(),\n","    \"RNN_atten_10-fold\": RNN_attenModel(), \"Transformer_10-fold\": TransformerModel(),\n","}\n","\n","for key in list(predictor.keys()):\n","    neg_folder = \"neg_10\" if \"_10-fold\" in key else \"neg\"\n","    arch_name = key.replace(\"_10-fold\", \"\")\n","\n","    # Path setup\n","    path = f\"./Ensemble_Storage/{arch_name}/{neg_folder}/LR_{best_run_lr[key]}/best_model.pth\"\n","\n","    if os.path.exists(path):\n","        checkpoint = torch.load(path, map_location=device)\n","        # Extract state dict (handling different save formats)\n","        state_dict = checkpoint['model_state_dict'] if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint else checkpoint\n","\n","        predictor[key].load_state_dict(state_dict)\n","        predictor[key].to(device) # Move entire model to GPU/CPU\n","        predictor[key].eval()\n","    else:\n","        print(f\"‚ö†Ô∏è Warning: Checkpoint missing for {key} at {path}. Removing from ensemble.\")\n","        del predictor[key]\n","\n","# --- 2. PRE-CALCULATE PREDICTIONS ---\n","# Important: shuffle=False ensures labels and predictions align perfectly\n","test_loader = DataLoader(test_set, batch_size=256, shuffle=False, pin_memory=True)\n","\n","print(f\"üöÄ Performing inference on {len(predictor)} models...\")\n","model_outputs = {name: [] for name in predictor.keys()}\n","labels_all = []\n","\n","\n","\n","with torch.no_grad():\n","    for data, labels in tqdm(test_loader, desc=\"Inference\"):\n","        labels_all.append(labels.numpy())\n","        data_gpu = data.to(device)\n","\n","        for name, model in predictor.items():\n","            output = model(data_gpu)\n","            # Use Softmax to get probabilities for Soft Voting\n","            probs = torch.softmax(output, dim=1).cpu().numpy()\n","            model_outputs[name].append(probs)\n","\n","# Flatten to single matrices\n","labels_all = np.concatenate(labels_all)\n","for name in model_outputs:\n","    model_outputs[name] = np.concatenate(model_outputs[name])\n","\n","# --- 3. FAST COMBINATION TESTING ---\n","print(\"üó≥Ô∏è Testing all ensemble combinations (CPU Optimized)...\")\n","results_list = []\n","all_model_names = list(predictor.keys())\n","\n","for vote_type in [True, False]: # True = Hard Voting, False = Soft Voting\n","    mode = \"Hard\" if vote_type else \"Soft\"\n","    for i in range(2, len(all_model_names) + 1):\n","        for combo in combinations(all_model_names, i):\n","            # Stack outputs of selected models: [num_models, num_samples, 2]\n","            combo_stack = np.stack([model_outputs[m] for m in combo])\n","\n","            if vote_type: # HARD VOTING (Majority Class)\n","                predictions = np.argmax(combo_stack, axis=2)\n","                # Correct majority: sum of '1's > half the size of the combo\n","                final_pred = (np.sum(predictions, axis=0) > (len(combo) / 2)).astype(int)\n","            else: # SOFT VOTING (Average Probability)\n","                avg_probs = np.mean(combo_stack, axis=0)\n","                final_pred = np.argmax(avg_probs, axis=1)\n","\n","            # Calculate scores\n","            acc = metrics.accuracy_score(labels_all, final_pred)\n","            auc = metrics.roc_auc_score(labels_all, final_pred)\n","\n","            results_list.append({\n","                \"combo\": \"|\".join(combo),\n","                \"vote_type\": mode,\n","                \"num_models\": len(combo),\n","                \"acc\": acc,\n","                \"auc\": auc\n","            })\n","\n","# --- 4. SUMMARY ---\n","results_df = pd.DataFrame(results_list)\n","results_df.to_csv(\"ensemble_optimization_results.csv\", index=False)\n","\n","# Identify winners\n","best_acc_row = results_df.loc[results_df['acc'].idxmax()]\n","print(\"\\n\" + \"=\"*40)\n","print(f\"üèÜ BEST ACCURACY ENSEMBLE:\")\n","print(f\"Models: {best_acc_row['combo']}\")\n","print(f\"Method: {best_acc_row['vote_type']} Voting\")\n","print(f\"Accuracy: {best_acc_row['acc']:.4f}\")\n","print(\"=\"*40)"],"metadata":{"id":"XaQjhRvBvnWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate_seq_path = \"/content/drive/MyDrive/AMP-Generation/data/generated_amp_sequences.txt\"\n","model = ['CNN_10-fold', 'RCNN_10-fold', 'RNN_10-fold', 'Transformer_10-fold']  #select best combine model"],"metadata":{"id":"McU1XTMHvych","executionInfo":{"status":"ok","timestamp":1770115099464,"user_tz":-240,"elapsed":22,"user":{"displayName":"Tamar Vanishvili","userId":"05982125587332184201"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import os\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","from src.Transformer import peptide_tokenizer, encode_seq\n","\n","\n","from src.RCNN import RCNNModel\n","from src.CNN import CNNModel\n","from src.RNN import RNNModel\n","from src.RNN_atten import RNN_attenModel\n","from src.Transformer import TransformerModel\n","\n","class MyDataset_pred(Dataset):\n","    def __init__(self, file_path):\n","        super().__init__()\n","        # Check if file exists to avoid another error\n","        if not os.path.exists(file_path):\n","            raise FileNotFoundError(f\"Could not find the sequence file at: {file_path}\")\n","\n","        self.data = pd.read_csv(file_path, header=None)\n","        self.num = len(self.data)\n","        self.data = self.data.values.tolist()\n","        self.max_length = 64\n","\n","    def __len__(self):\n","        return self.num\n","\n","    def __getitem__(self, index):\n","        seq = self.data[index]\n","        # These functions must be imported or defined in your notebook\n","        seq_list = peptide_tokenizer(seq[0])\n","        encoded_seq = encode_seq(seq_list, self.max_length - 1, w2i)\n","        encoded_seq = [0] + encoded_seq\n","        return torch.tensor(encoded_seq), seq[0]\n","\n","# --- 1. INITIALIZE & DYNAMICALLY LOAD MODELS ---\n","predictor = {\n","    \"CNN\": CNNModel(), \"RCNN\": RCNNModel(), \"RNN\": RNNModel(),\n","    \"RNN_atten\": RNN_attenModel(), \"Transformer\": TransformerModel(),\n","    \"CNN_10-fold\": CNNModel(), \"RCNN_10-fold\": RCNNModel(), \"RNN_10-fold\": RNNModel(),\n","    \"RNN_atten_10-fold\": RNN_attenModel(), \"Transformer_10-fold\": TransformerModel(),\n","}\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","for key in predictor.keys():\n","    # 1. Determine the folder (neg vs neg_10)\n","    neg_folder = \"neg_10\" if \"_10-fold\" in key else \"neg\"\n","    # 2. Determine the base architecture name\n","    arch_name = key.replace(\"_10-fold\", \"\")\n","    # 3. Get the winning Learning Rate\n","    lr = best_run_lr[key]\n","\n","    # NEW UPGRADED PATH: Matches your Ensemble_Storage structure\n","    model_path = f\"./Ensemble_Storage/{arch_name}/{neg_folder}/LR_{lr}/best_model.pth\"\n","\n","    if os.path.exists(model_path):\n","        checkpoint = torch.load(model_path, map_location=device)\n","        # Handle dict format vs raw state_dict\n","        state_dict = checkpoint['model_state_dict'] if isinstance(checkpoint, dict) else checkpoint\n","\n","        predictor[key].load_state_dict(state_dict)\n","        predictor[key].to(device)\n","        predictor[key].eval()\n","        print(f\"‚úÖ Loaded: {key} from {model_path}\")\n","    else:\n","        print(f\"‚ùå Error: Could not find {model_path}\")\n","\n","# --- 2. THE FILTERING PROCESS ---\n","test_set = MyDataset_pred(generate_seq_path)\n","test_loader = DataLoader(test_set, batch_size=256, shuffle=False)\n","\n","pos_seq = []\n","print(f\"üöÄ Filtering sequences from {generate_seq_path}...\")\n","\n","with torch.no_grad():\n","    for data, seqs in tqdm(test_loader):\n","        # We use all 10 models for the vote (or your winning combination list)\n","        model_names = list(predictor.keys())\n","        output = predict(data.to(device), model_names, predictor, vote=True)\n","\n","        # Indices where the ensemble said \"0\" (AMP)\n","        indices = np.where(output == 0)[0]\n","\n","        # Extract those specific text sequences\n","        valid_batch = np.array(seqs)[indices].tolist()\n","        pos_seq.extend(valid_batch)\n","\n","# Final cleanup\n","pos_seq = list(set(pos_seq))\n","print(f\"Found {len(pos_seq)} unique high-quality AMP candidates.\")\n","\n","# Write to file\n","with open(\"./generated_amp_filtered.txt\", \"w\") as wf:\n","    for seq in pos_seq:\n","        wf.write(f\"{seq}\\n\")"],"metadata":{"id":"1DiWGZzuvz-f","colab":{"base_uri":"https://localhost:8080/","height":257,"referenced_widgets":["0fafbd73c73e4af8b6c047d3a7e82238","cb014f50e0ad40c08aac2aa5b2089baf","2af697f326f441e794e2d655958d35fd","39dc1d6563054db3a95fdea57967827b","908191413e614728acdf8ba0fc67248c","6e72965d207940e3ba0557bc1e21585a","ea7159d4e0634aa2912221d5f26ef07f","98fad9ba2d69415ab725286b23590c8d","bdb39226a51a4e92a9b2357652227ff2","c105eb1c89ad478c90156059593f80c0","2ff14b9099104e3fa391385ab9d5cd5b"]},"executionInfo":{"status":"ok","timestamp":1770115363459,"user_tz":-240,"elapsed":7673,"user":{"displayName":"Tamar Vanishvili","userId":"05982125587332184201"}},"outputId":"9185f83c-69cc-4ab5-fdfb-1004a16e4b39"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Loaded: CNN from ./Ensemble_Storage/CNN/neg/LR_0.001/best_model.pth\n","‚úÖ Loaded: RCNN from ./Ensemble_Storage/RCNN/neg/LR_0.0001/best_model.pth\n","‚úÖ Loaded: RNN from ./Ensemble_Storage/RNN/neg/LR_0.0001/best_model.pth\n","‚úÖ Loaded: RNN_atten from ./Ensemble_Storage/RNN_atten/neg/LR_0.001/best_model.pth\n","‚úÖ Loaded: Transformer from ./Ensemble_Storage/Transformer/neg/LR_0.001/best_model.pth\n","‚úÖ Loaded: CNN_10-fold from ./Ensemble_Storage/CNN/neg_10/LR_0.001/best_model.pth\n","‚úÖ Loaded: RCNN_10-fold from ./Ensemble_Storage/RCNN/neg_10/LR_0.001/best_model.pth\n","‚úÖ Loaded: RNN_10-fold from ./Ensemble_Storage/RNN/neg_10/LR_0.001/best_model.pth\n","‚úÖ Loaded: RNN_atten_10-fold from ./Ensemble_Storage/RNN_atten/neg_10/LR_0.001/best_model.pth\n","‚úÖ Loaded: Transformer_10-fold from ./Ensemble_Storage/Transformer/neg_10/LR_0.0001/best_model.pth\n","üöÄ Filtering sequences from /content/drive/MyDrive/AMP-Generation/data/generated_amp_sequences.txt...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/20 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fafbd73c73e4af8b6c047d3a7e82238"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Found 2206 unique high-quality AMP candidates.\n"]}]},{"cell_type":"code","source":["def remove_konw(data, know_data):\n","    same_data = pd.merge(data, know_data, how=\"inner\")\n","    outdata = data[~data.isin(same_data)].dropna()\n","    return outdata\n","def remove_c(data):\n","    data = data[~data[0].str.contains(\"C\")]\n","    return data\n","def seq_check_pos_charge(seq):\n","    for i in range(len(seq)-4):\n","        tem_seq = seq[i:i+5]\n","        num = tem_seq.count(\"K\") + tem_seq.count(\"R\")\n","        if num > 3:\n","            return False\n","    return True\n","def remove_pos_charge(data):\n","    data = data[data[0].apply(seq_check_pos_charge)]\n","    return data\n","def seq_check_hydrophobic(seq):\n","    for i in range(len(seq)-2):\n","        tem_seq = seq[i:i+3]\n","        num = tem_seq.count(\"F\") + tem_seq.count(\"V\") + \\\n","              tem_seq.count(\"I\") + tem_seq.count(\"W\") + \\\n","              tem_seq.count(\"L\") + tem_seq.count(\"A\") + \\\n","              tem_seq.count(\"M\")\n","        if num == 3:\n","            return False\n","    return True\n","def remove_hydrophobic(data):\n","    data = data[data[0].apply(seq_check_hydrophobic)]\n","    return data\n","def seq_check_repeat_three(seq):\n","    for i in range(len(seq)-2):\n","        tem_seq = seq[i:i+3]\n","        # print(tem_seq)g\n","        if tem_seq[0] == tem_seq[1] and tem_seq[0] == tem_seq[2]:\n","            return False\n","    return True\n","def remove_repeat_three(data):\n","    data = data[data[0].apply(seq_check_repeat_three)]\n","    return data\n","\n","\n","data = pd.read_csv(f\"/content/drive/MyDrive/AMP-Generation/data/generated_amp_filtered.txt\",header=None)\n","know_data = pd.read_csv(f\"params/pos_data\",header=None)\n","data = remove_konw(data,know_data)\n","data = remove_c(data)\n","data = remove_pos_charge(data)\n","data = remove_hydrophobic(data)\n","data = remove_repeat_three(data)\n","data.to_csv(f\"/content/drive/MyDrive/AMP-Generation/data/generated_amp_filtered2.txt\",header= False,index = False)"],"metadata":{"id":"H3T9fL2Cv-yD","executionInfo":{"status":"ok","timestamp":1770115635905,"user_tz":-240,"elapsed":219,"user":{"displayName":"Tamar Vanishvili","userId":"05982125587332184201"}}},"execution_count":16,"outputs":[]}]}