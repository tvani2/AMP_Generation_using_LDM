{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOgKY9cybDK4YsHb6cWPBdQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IoX2oik8j94g","executionInfo":{"status":"ok","timestamp":1769804923328,"user_tz":-240,"elapsed":60660,"user":{"displayName":"Tamar Vanishvili","userId":"05982125587332184201"}},"outputId":"d7de42ac-213c-4441-bdff-64553e004810"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Generated 5120 AMP latent vectors\n","Saved to: /content/drive/MyDrive/AMP-Generation/data/generated_latent_amp.pth\n"]}],"source":["# Conditional Reverse Diffusion Sampling\n","\n","import torch\n","import torch.nn as nn\n","import math\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","MODEL_PATH = \"/content/drive/MyDrive/AMP-Generation/models/diffusion_conditional_final.pth\"\n","SAVE_PATH  = \"/content/drive/MyDrive/AMP-Generation/data/generated_latent_amp.pth\"\n","\n","# Diffusion hyperparameters\n","T = 500\n","beta_start = 1e-4\n","beta_end   = 0.02\n","\n","betas = torch.linspace(beta_start, beta_end, T, device=device)\n","alphas = 1.0 - betas\n","alphas_bar = torch.cumprod(alphas, dim=0)\n","\n","# ᾱ_{t-1}\n","alphas_bar_prev = torch.cat(\n","    [torch.tensor([1.0], device=device), alphas_bar[:-1]],\n","    dim=0\n",")\n","\n","# β̃_t (posterior variance)\n","beta_tilde = (1.0 - alphas_bar_prev) / (1.0 - alphas_bar) * betas\n","\n","# Time embedding\n","class TimeEmbedding(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, dim * 4),\n","            nn.SiLU(),\n","            nn.Linear(dim * 4, dim)\n","        )\n","\n","    def forward(self, t):\n","        half = self.dim // 2\n","        scale = math.log(10000) / (half - 1)\n","        emb = torch.exp(torch.arange(half, device=t.device) * -scale)\n","        emb = t[:, None] * emb[None, :]\n","        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n","        return self.mlp(emb)\n","\n","# Conditional Latent Diffusion Model\n","class LatentDiffusion(nn.Module):\n","    def __init__(self, latent_dim=64, hidden_dim=512, num_classes=2):\n","        super().__init__()\n","\n","        self.time_embed = TimeEmbedding(hidden_dim)\n","        self.cond_embed = nn.Embedding(num_classes, hidden_dim)\n","        self.fc_in = nn.Linear(latent_dim, hidden_dim)\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=hidden_dim,\n","            nhead=8,\n","            dim_feedforward=2048,\n","            dropout=0.1,\n","            batch_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(\n","            encoder_layer,\n","            num_layers=6\n","        )\n","\n","        self.norm = nn.LayerNorm(hidden_dim)\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc_out = nn.Linear(hidden_dim, latent_dim)\n","\n","    def forward(self, x, t, y):\n","        h = self.fc_in(x)\n","        h = h + self.time_embed(t) + self.cond_embed(y)\n","        h = self.norm(h)\n","        h = self.dropout(h)\n","        h = self.transformer(h.unsqueeze(1)).squeeze(1)\n","        return self.fc_out(h)  # predicts X0\n","\n","\n","model = LatentDiffusion().to(device)\n","checkpoint = torch.load(MODEL_PATH, map_location=device)\n","\n","if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","else:\n","    model.load_state_dict(checkpoint)\n","\n","model.eval()\n","\n","# Reverse diffusion sampling\n","@torch.no_grad()\n","def conditional_reverse_sampling(\n","    model,\n","    batch_size=512,\n","    num_batches=10,\n","    class_label=1  # AMP\n","):\n","    all_samples = []\n","\n","    for _ in range(num_batches):\n","        # X_T ~ N(0, I)\n","        xt = torch.randn(batch_size, 64, device=device)\n","        y = torch.full((batch_size,), class_label, dtype=torch.long, device=device)\n","\n","        for t in reversed(range(T)):\n","            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n","\n","            # Predict X0\n","            x0_pred = model(xt, t_tensor, y)\n","\n","            # μ̃(Xt, X0) — paper equation\n","            coef1 = (\n","                torch.sqrt(alphas_bar_prev[t]) * betas[t]\n","                / (1.0 - alphas_bar[t])\n","            )\n","            coef2 = (\n","                torch.sqrt(alphas[t]) * (1.0 - alphas_bar_prev[t])\n","                / (1.0 - alphas_bar[t])\n","            )\n","\n","            mean = coef1 * x0_pred + coef2 * xt\n","\n","            if t > 0:\n","                noise = torch.randn_like(xt)\n","                xt = mean + torch.sqrt(beta_tilde[t])* noise\n","            else:\n","                xt = mean  # t = 0, no noise\n","\n","        all_samples.append(xt.cpu())\n","\n","    return torch.cat(all_samples, dim=0)\n","\n","# Run generation\n","generated_latents = conditional_reverse_sampling(\n","    model=model,\n","    batch_size=512,\n","    num_batches=10,\n","    class_label=1\n",")\n","\n","torch.save(generated_latents, SAVE_PATH)\n","\n","print(f\"Generated {generated_latents.shape[0]} AMP latent vectors\")\n","print(f\"Saved to: {SAVE_PATH}\")"]}]}