{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch  # Core PyTorch library for tensors and GPU acceleration\n",
        "import torch.nn as nn  # Neural network layers\n",
        "import torch.optim as optim  # Optimization algorithms (not used here but imported)\n",
        "import pandas as pd  # For reading CSV data\n",
        "from torch.utils.data import DataLoader, TensorDataset  # Create dataset + loader combos\n",
        "from torch.nn.utils.rnn import pack_padded_sequence  # Helps GRUs handle variable-length sequences\n",
        "from tqdm import tqdm  # Progress bars because suffering should be visible\n",
        "import os  # File handling because computers need directions\n",
        "from google.colab import drive  # Mount Google Drive for I/O\n",
        "\n",
        "drive.mount('/content/drive')  # Plug Drive into the Colab robot\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Pick GPU if blessed by the silicon gods\n",
        "\n",
        "config = {\n",
        "    \"max_length\": 50,  # Max sequence length after padding\n",
        "    \"embedding_dim\": 128,  # Size of learned amino acid embeddings\n",
        "    \"hidden_dim\": 512,  # GRU hidden size (big brain mode)\n",
        "    \"latent_dim\": 64,  # Size of VAE latent vector\n",
        "    \"batch_size\": 4096  # Huge batch cause this is just encoding\n",
        "}\n",
        "\n",
        "class PeptideTokenizer:\n",
        "    def __init__(self):\n",
        "        self.chars = ['<PAD>', '<SOS>', '<EOS>', '<UNK>'] + list(\"ACDEFGHIKLMNPQRSTVWY\")  # Token inventory\n",
        "        self.char_to_idx = {c: i for i, c in enumerate(self.chars)}  # Map char -> ID\n",
        "        self.idx_to_char = {i: c for i, c in enumerate(self.chars)}  # Map ID -> char\n",
        "        self.vocab_size = len(self.chars)  # Number of tokens\n",
        "\n",
        "    def encode_batch(self, seqs, max_len):\n",
        "        batch_tensor = torch.full((len(seqs), max_len), self.char_to_idx['<PAD>'], dtype=torch.long)  # Pre-fill with PADs\n",
        "        lengths = []  # Store each sequence length\n",
        "        for i, seq in enumerate(tqdm(seqs, desc=\"Tokenizing\")):\n",
        "            s = seq[:max_len - 2]  # Account for SOS and EOS\n",
        "            idx = [self.char_to_idx.get(aa, self.char_to_idx['<UNK>']) for aa in s]  # Convert string -> indices\n",
        "            full_seq = [self.char_to_idx['<SOS>']] + idx + [self.char_to_idx['<EOS>']]  # Add special tokens\n",
        "            length = len(full_seq)\n",
        "            batch_tensor[i, :length] = torch.tensor(full_seq)  # Insert actual sequence\n",
        "            lengths.append(length)  # Track true length\n",
        "        return batch_tensor, torch.tensor(lengths, dtype=torch.long)  # Return padded data + lengths\n",
        "\n",
        "    def decode(self, indices):\n",
        "        res = []  # Collect characters\n",
        "        for idx in indices:\n",
        "            if idx == self.char_to_idx['<EOS>']: break  # Stop at EOS\n",
        "            if idx in [self.char_to_idx['<SOS>'], self.char_to_idx['<PAD>']]: continue  # Skip fluff tokens\n",
        "            res.append(self.idx_to_char[idx])  # Convert back to character\n",
        "        return \"\".join(res)  # Return nice string\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)  # Turn ints into vectors\n",
        "        self.encoder_gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)  # BiGRU encoder\n",
        "        self.fc_mu = nn.Linear(hidden_dim * 2, latent_dim)  # Mean of latent Gaussian\n",
        "        self.fc_logvar = nn.Linear(hidden_dim * 2, latent_dim)  # Log variance (unused in this trimmed version)\n",
        "        self.decoder_input = nn.Linear(latent_dim, hidden_dim)  # Latent -> GRU init state\n",
        "        self.decoder_gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)  # Decoder GRU\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)  # Output token distribution\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        embedded = self.embedding(x)  # Convert tokens to vectors\n",
        "        packed_input = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)  # Compress padding\n",
        "        _, hidden = self.encoder_gru(packed_input)  # GRU returns hidden states\n",
        "        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)  # Combine forward + backward GRU ends\n",
        "        mu = self.fc_mu(hidden_cat)  # Compute latent mean only\n",
        "        return mu  # For fast inference\n",
        "\n",
        "MODEL_PATH = \"/content/drive/MyDrive/AMP-Generation/models/vae_FINAL_epoch20.pth\"  # Saved VAE weights\n",
        "STATS_PATH = \"/content/drive/MyDrive/AMP-Generation/models/vae_correction_stats.pth\"  # Scaling stats\n",
        "POS_DATA   = \"/content/drive/MyDrive/AMP-Generation/data/pos_data.csv\"  # Positive samples\n",
        "NEG_DATA   = \"/content/drive/MyDrive/AMP-Generation/data/neg_data.csv\"  # Negative samples\n",
        "\n",
        "tokenizer = PeptideTokenizer()  # Tokenizer instance\n",
        "model = VAE(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    emb_dim=config[\"embedding_dim\"],\n",
        "    hidden_dim=config[\"hidden_dim\"],\n",
        "    latent_dim=config[\"latent_dim\"]\n",
        ").to(device)  # Move model to GPU if possible\n",
        "\n",
        "checkpoint = torch.load(MODEL_PATH, map_location=device)  # Load model checkpoint\n",
        "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])  # Standard training save format\n",
        "else:\n",
        "    model.load_state_dict(checkpoint)  # Raw state dict\n",
        "model.eval()  # Switch to inference mode\n",
        "\n",
        "# Load stats\n",
        "stats = torch.load(STATS_PATH, map_location=device)  # Load normalization stats\n",
        "mu_mean = stats['shift'].to(device)  # Mean for standardization\n",
        "mu_std = stats['scale'].to(device)  # Std deviation\n",
        "\n",
        "def extract_and_normalize(csv_path, output_filename):\n",
        "    print(f\"Converting {os.path.basename(csv_path)} to Latent Space...\")  # Progress update\n",
        "    df = pd.read_csv(csv_path)  # Load dataset\n",
        "    col = 'sequence' if 'sequence' in df.columns else df.columns[0]  # Determine column name\n",
        "    seqs = df[col].astype(str).tolist()  # Convert to string list\n",
        "    data_tensor, lengths_tensor = tokenizer.encode_batch(seqs, config[\"max_length\"])  # Tokenize batch\n",
        "    loader = DataLoader(TensorDataset(data_tensor, lengths_tensor), batch_size=config[\"batch_size\"], shuffle=False)  # Create loader\n",
        "\n",
        "    latents_list = []  # Collect output tensors\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for batch, lengths in tqdm(loader):  # Iterate over batches\n",
        "            batch = batch.to(device)  # Move to GPU\n",
        "            mu = model(batch, lengths)  # Encode into latent mean\n",
        "            norm_mu = (mu - mu_mean) / mu_std  # Normalize using saved stats\n",
        "            latents_list.append(norm_mu.cpu())  # Store on CPU\n",
        "\n",
        "    final_tensor = torch.cat(latents_list, dim=0)  # Combine all latent vectors\n",
        "    save_path = os.path.join(\"/content/drive/MyDrive/AMP-Generation/data/\", output_filename)  # Output path\n",
        "    torch.save(final_tensor, save_path)  # Save tensor to file\n",
        "    print(f\"Success! Saved to {save_path}\")  # Confirmation\n",
        "\n",
        "extract_and_normalize(POS_DATA, \"latent_cond_pos.pth\")  # Process positive dataset\n",
        "extract_and_normalize(NEG_DATA, \"latent_cond_neg.pth\")  # Process negative dataset\n"
      ],
      "metadata": {
        "id": "9CNf3gFqx86p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}