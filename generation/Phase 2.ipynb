{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPqHK0d3Ag6bTBfetk0tJFZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O3WOz30Wa8Vv","executionInfo":{"status":"ok","timestamp":1769802975670,"user_tz":-240,"elapsed":247638,"user":{"displayName":"Tamar Vanishvili","userId":"05982125587332184201"}},"outputId":"807bd315-0f1b-47d0-ed0e-be8d7e616e8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 001 | Loss: 2.065861\n","--- Checkpoint saved: /content/drive/MyDrive/AMP-Generation/checkpoints/cond_diffusion_epoch_0.pth ---\n","Epoch 002 | Loss: 1.438597\n","Epoch 003 | Loss: 1.359149\n","Epoch 004 | Loss: 1.321990\n","Epoch 005 | Loss: 1.297335\n","Epoch 006 | Loss: 1.279717\n","--- Checkpoint saved: /content/drive/MyDrive/AMP-Generation/checkpoints/cond_diffusion_epoch_5.pth ---\n","Epoch 007 | Loss: 1.270250\n","Epoch 008 | Loss: 1.256148\n","Epoch 009 | Loss: 1.246658\n","Epoch 010 | Loss: 1.245291\n","Epoch 011 | Loss: 1.237857\n","--- Checkpoint saved: /content/drive/MyDrive/AMP-Generation/checkpoints/cond_diffusion_epoch_10.pth ---\n","Epoch 012 | Loss: 1.232610\n","Epoch 013 | Loss: 1.221161\n","Epoch 014 | Loss: 1.224280\n","Epoch 015 | Loss: 1.224826\n","Epoch 016 | Loss: 1.211987\n","--- Checkpoint saved: /content/drive/MyDrive/AMP-Generation/checkpoints/cond_diffusion_epoch_15.pth ---\n","Epoch 017 | Loss: 1.212359\n","Epoch 018 | Loss: 1.205418\n","Epoch 019 | Loss: 1.205405\n","Epoch 020 | Loss: 1.199057\n","Epoch 021 | Loss: 1.195894\n","--- Checkpoint saved: /content/drive/MyDrive/AMP-Generation/checkpoints/cond_diffusion_epoch_20.pth ---\n","Epoch 022 | Loss: 1.195720\n","Epoch 023 | Loss: 1.193692\n","Epoch 024 | Loss: 1.197619\n","Epoch 025 | Loss: 1.189981\n","Epoch 026 | Loss: 1.182479\n","--- Checkpoint saved: /content/drive/MyDrive/AMP-Generation/checkpoints/cond_diffusion_epoch_25.pth ---\n","Epoch 027 | Loss: 1.182037\n","Epoch 028 | Loss: 1.182721\n","Epoch 029 | Loss: 1.185915\n","Epoch 030 | Loss: 1.174494\n","Epoch 031 | Loss: 1.177678\n","--- Checkpoint saved: /content/drive/MyDrive/AMP-Generation/checkpoints/cond_diffusion_epoch_30.pth ---\n","Epoch 032 | Loss: 1.177952\n","Epoch 033 | Loss: 1.176553\n","Epoch 034 | Loss: 1.172604\n","Epoch 035 | Loss: 1.174641\n","Epoch 036 | Loss: 1.162471\n","--- Checkpoint saved: /content/drive/MyDrive/AMP-Generation/checkpoints/cond_diffusion_epoch_35.pth ---\n","Epoch 037 | Loss: 1.163022\n","Epoch 038 | Loss: 1.160816\n","Epoch 039 | Loss: 1.169211\n","Epoch 040 | Loss: 1.167605\n","Epoch 041 | Loss: 1.166698\n","--- Checkpoint saved: /content/drive/MyDrive/AMP-Generation/checkpoints/cond_diffusion_epoch_40.pth ---\n","Epoch 042 | Loss: 1.158374\n","Epoch 043 | Loss: 1.151106\n","Epoch 044 | Loss: 1.155358\n","Epoch 045 | Loss: 1.151989\n","Epoch 046 | Loss: 1.153260\n","--- Checkpoint saved: /content/drive/MyDrive/AMP-Generation/checkpoints/cond_diffusion_epoch_45.pth ---\n","Epoch 047 | Loss: 1.151116\n","Epoch 048 | Loss: 1.151238\n","Epoch 049 | Loss: 1.145870\n","Epoch 050 | Loss: 1.152181\n","Saved conditional diffusion model to: /content/drive/MyDrive/AMP-Generation/models/diffusion_conditional_final.pth\n"]}],"source":["# Conditional Latent Diffusion Fine-Tuning\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","import math\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Diffusion hyperparameters\n","T = 500\n","beta_start = 1e-4\n","beta_end = 0.02\n","\n","betas = torch.linspace(beta_start, beta_end, T, device=device)\n","alphas = 1.0 - betas\n","alphas_bar = torch.cumprod(alphas, dim=0)\n","\n","# Time embedding\n","class TimeEmbedding(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, dim * 4),\n","            nn.SiLU(),\n","            nn.Linear(dim * 4, dim)\n","        )\n","\n","    def forward(self, t):\n","        half = self.dim // 2\n","        scale = math.log(10000) / (half - 1)\n","        emb = torch.exp(torch.arange(half, device=t.device) * -scale)\n","        emb = t[:, None] * emb[None, :]\n","        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n","        return self.mlp(emb)\n","\n","# Conditional Latent Diffusion Model\n","class LatentDiffusion(nn.Module):\n","    def __init__(self, latent_dim=64, hidden_dim=512, num_classes=2):\n","        super().__init__()\n","\n","        self.time_embed = TimeEmbedding(hidden_dim)\n","        self.cond_embed = nn.Embedding(num_classes, hidden_dim)\n","\n","        self.fc_in = nn.Linear(latent_dim, hidden_dim)\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=hidden_dim,\n","            nhead=8,\n","            dim_feedforward=2048,\n","            dropout=0.1,\n","            batch_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(\n","            encoder_layer,\n","            num_layers=6\n","        )\n","\n","        self.norm = nn.LayerNorm(hidden_dim)\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc_out = nn.Linear(hidden_dim, latent_dim)\n","\n","    def forward(self, x, t, y):\n","        h = self.fc_in(x)\n","        h = h + self.time_embed(t) + self.cond_embed(y)\n","        h = self.norm(h)\n","        h = self.dropout(h)\n","        h = self.transformer(h.unsqueeze(1)).squeeze(1)\n","        return self.fc_out(h)\n","\n","# Forward diffusion (q)\n","def q_sample(x0, t, noise):\n","    a_bar = alphas_bar[t].unsqueeze(1)\n","    return torch.sqrt(a_bar) * x0 + torch.sqrt(1.0 - a_bar) * noise\n","\n","\n","UNCOND_MODEL_PATH = \"/content/drive/MyDrive/AMP-Generation/checkpoints/diffusion_paper_final_ep100.pth\"\n","POS_LATENT_PATH = \"/content/drive/MyDrive/AMP-Generation/data/latent_cond_pos.pth\"\n","NEG_LATENT_PATH = \"/content/drive/MyDrive/AMP-Generation/data/latent_cond_neg.pth\"\n","SAVE_PATH = \"/content/drive/MyDrive/AMP-Generation/models/diffusion_conditional_final.pth\"\n","CHECKPOINT_DIR = \"/content/drive/MyDrive/AMP-Generation/checkpoints\"\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n","\n","\n","# Load model (from unconditional)\n","model = LatentDiffusion().to(device)\n","checkpoint = torch.load(UNCOND_MODEL_PATH, map_location=device)\n","\n","if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n","    model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n","else:\n","    model.load_state_dict(checkpoint, strict=False)\n","\n","model.train()\n","\n","# Load conditional latent data\n","pos_latent = torch.load(POS_LATENT_PATH)\n","neg_latent = torch.load(NEG_LATENT_PATH)\n","\n","pos_labels = torch.ones(pos_latent.size(0), dtype=torch.long)\n","neg_labels = torch.zeros(neg_latent.size(0), dtype=torch.long)\n","\n","latents = torch.cat([pos_latent, neg_latent], dim=0)\n","labels = torch.cat([pos_labels, neg_labels], dim=0)\n","\n","dataset = TensorDataset(latents, labels)\n","loader = DataLoader(\n","    dataset,\n","    batch_size=512,\n","    shuffle=True,\n","    drop_last=True\n",")\n","\n","# Optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n","\n","# Training loop\n","EPOCHS = 50\n","CHECKPOINT_INTERVAL = 5\n","\n","for epoch in range(EPOCHS):\n","    total_loss = 0.0\n","\n","    for x0, y in loader:\n","        x0 = x0.to(device)\n","        y = y.to(device)\n","\n","        t = torch.randint(0, T, (x0.size(0),), device=device)\n","        noise = torch.randn_like(x0)\n","\n","        xt = q_sample(x0, t, noise)\n","        x_pred = model(xt, t, y)\n","\n","        loss = F.mse_loss(x_pred, x0)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(loader)\n","    print(f\"Epoch {epoch+1:03d} | Loss: {avg_loss:.6f}\")\n","\n","    # Save Checkpoint every 5 epochs\n","    if epoch % CHECKPOINT_INTERVAL == 0:\n","        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"cond_diffusion_epoch_{epoch}.pth\")\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': avg_loss,\n","        }, ckpt_path)\n","        print(f\"--- Checkpoint saved: {ckpt_path} ---\")\n","\n","# Save conditional model\n","torch.save(\n","    {\"model_state_dict\": model.state_dict()},\n","    SAVE_PATH\n",")\n","\n","print(f\"Saved conditional diffusion model to: {SAVE_PATH}\")\n"]}]}