{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkoko22/AMP_Generation_using_LDM/blob/main/generation/Decoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGwvpsI3d0R6",
        "outputId": "c036b62c-3a93-4ee6-ab3e-58cd592fecf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "VAE loaded successfully.\n",
            "Loaded 5120 latent vectors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Decoding: 100%|██████████| 5/5 [00:00<00:00,  9.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded 5120 sequences\n",
            "Saved to: /content/drive/MyDrive/AMP-Generation/data/generated_amp_sequences.txt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "LATENT_PATH = \"/content/drive/MyDrive/AMP-Generation/data/generated_latent_amp_denorm.pth\"\n",
        "VAE_PATH    = \"/content/drive/MyDrive/AMP-Generation/checkpoints/vae_FINAL_epoch20.pth\"\n",
        "SAVE_PATH   = \"/content/drive/MyDrive/AMP-Generation/data/generated_amp_sequences.txt\"\n",
        "\n",
        "MAX_LEN = 50\n",
        "EMB_DIM = 128\n",
        "HIDDEN_DIM = 512\n",
        "LATENT_DIM = 64\n",
        "\n",
        "\n",
        "class PeptideTokenizer:\n",
        "    def __init__(self):\n",
        "        self.chars = ['<PAD>', '<SOS>', '<EOS>', '<UNK>'] + list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "        self.char_to_idx = {c: i for i, c in enumerate(self.chars)}\n",
        "        self.idx_to_char = {i: c for i, c in enumerate(self.chars)}\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "    def decode(self, indices):\n",
        "        res = []\n",
        "        for idx in indices:\n",
        "            if idx == self.char_to_idx['<EOS>']:\n",
        "                break\n",
        "            if idx in (self.char_to_idx['<PAD>'], self.char_to_idx['<SOS>']):\n",
        "                continue\n",
        "            res.append(self.idx_to_char[idx])\n",
        "        return \"\".join(res)\n",
        "\n",
        "tokenizer = PeptideTokenizer()\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, latent_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        # encoder\n",
        "        self.encoder_gru = nn.GRU(\n",
        "            emb_dim, hidden_dim,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(hidden_dim * 2, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim * 2, latent_dim)\n",
        "\n",
        "        # decoder\n",
        "        self.decoder_input = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.decoder_gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def decode_latent(self, z, max_len=50):\n",
        "        batch_size = z.size(0)\n",
        "        hidden = self.decoder_input(z).unsqueeze(0)\n",
        "\n",
        "        inputs = torch.full(\n",
        "            (batch_size, 1),\n",
        "            tokenizer.char_to_idx['<SOS>'],\n",
        "            device=z.device,\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        generated = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            emb = self.embedding(inputs[:, -1:])\n",
        "            out, hidden = self.decoder_gru(emb, hidden)\n",
        "            logits = self.fc_out(out.squeeze(1))\n",
        "            next_token = torch.argmax(logits, dim=-1)\n",
        "            inputs = torch.cat([inputs, next_token.unsqueeze(1)], dim=1)\n",
        "            generated.append(next_token)\n",
        "\n",
        "        return torch.stack(generated, dim=1)\n",
        "\n",
        "\n",
        "model = VAE(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    emb_dim=EMB_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    latent_dim=LATENT_DIM\n",
        ").to(device)\n",
        "\n",
        "state = torch.load(VAE_PATH, map_location=device)\n",
        "if isinstance(state, dict) and \"model_state_dict\" in state:\n",
        "    state = state[\"model_state_dict\"]\n",
        "\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "\n",
        "print(\"VAE loaded successfully.\")\n",
        "\n",
        "# Load latents\n",
        "z = torch.load(LATENT_PATH, map_location=device)\n",
        "print(f\"Loaded {z.shape[0]} latent vectors\")\n",
        "\n",
        "# Decode\n",
        "decoded_sequences = []\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "for i in tqdm(range(0, z.size(0), BATCH_SIZE), desc=\"Decoding\"):\n",
        "    batch_z = z[i:i + BATCH_SIZE].to(device)\n",
        "    tokens = model.decode_latent(batch_z, MAX_LEN).cpu().numpy()\n",
        "\n",
        "    for seq in tokens:\n",
        "        peptide = tokenizer.decode(seq)\n",
        "        if len(peptide) > 0:\n",
        "            decoded_sequences.append(peptide)\n",
        "\n",
        "# Save\n",
        "with open(SAVE_PATH, \"w\") as f:\n",
        "    for s in decoded_sequences:\n",
        "        f.write(s + \"\\n\")\n",
        "\n",
        "print(f\"Decoded {len(decoded_sequences)} sequences\")\n",
        "print(f\"Saved to: {SAVE_PATH}\")\n"
      ]
    }
  ]
}